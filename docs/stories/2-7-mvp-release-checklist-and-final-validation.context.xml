<story-context id="bmad/bmm/workflows/4-implementation/story-context/template" v="1.0">
  <metadata>
    <epicId>2</epicId>
    <storyId>7</storyId>
    <title>MVP Release Checklist & Final Validation</title>
    <status>drafted</status>
    <generatedAt>2025-11-08</generatedAt>
    <generator>BMAD Story Context Workflow</generator>
    <sourceStoryPath>docs/stories/2-7-mvp-release-checklist-and-final-validation.md</sourceStoryPath>
  </metadata>

  <story>
    <asA>project stakeholder</asA>
    <iWant>comprehensive validation of the complete MVP system</iWant>
    <soThat>we can confidently release a reliable, production-ready application</soThat>
    <tasks>
- Task 1: End-to-End Workflow Validation (AC: #1)
  - Prepare test media files: 5+ files (MP3, MP4, WAV, varying lengths: 10s, 5min, 1hr)
  - Test complete workflow on each file: Upload → Progress → Transcription Display
  - Test review workflow: Media playback, click-to-timestamp, active highlighting
  - Test editing workflow: Inline editing, Tab/Enter navigation, Escape revert
  - Test export workflow: Both SRT and TXT formats, verify downloads
  - Verify data flywheel: Check edited.json and export_metadata.json created
  - Document any issues found in validation report

- Task 2: Cross-Browser Testing (AC: #2, #9)
  - Test Chrome desktop (version 90+): Full workflow, all features
  - Test Firefox desktop (version 88+): Full workflow, all features
  - Test Safari desktop (version 14+): Full workflow, media seeking behavior
  - Test Edge desktop (version 90+): Full workflow, all features
  - Test Chrome mobile (Android/iOS): Responsive layout, touch interactions
  - Test Safari mobile (iOS): Responsive layout, touch interactions, media seeking
  - Document browser-specific issues and workarounds

- Task 3: Error Scenario Testing (AC: #3, #8)
  - Test unsupported file format upload (e.g., .exe, .pdf)
  - Test file size exceeded (upload >2GB file if possible, or mock)
  - Test network timeout during upload (throttle network, disconnect)
  - Test network timeout during transcription status polling
  - Test corrupted media file upload (truncated MP3, invalid headers)
  - Test export failure scenarios (network error, server error)
  - Test WhisperX model download failure (first-time setup, network issues)
  - Test concurrent job limits (start multiple uploads simultaneously)
  - Verify all error messages are user-friendly (no technical jargon/stack traces)

- Task 4: Performance Validation (AC: #4)
  - Measure UI load time: Homepage → Upload page (<3s per NFR001)
  - Measure media playback start time: Click play → audio starts (<2s per NFR001)
  - Measure click-to-timestamp response time: Click subtitle → player seeks (<1s per NFR001)
  - Test transcription processing time: 1 hour audio → verify 30-60 min processing (1-2x real-time)
  - Use Chrome DevTools Performance tab for measurements
  - Document performance metrics in validation report

- Task 5: Mobile Responsiveness Testing (AC: #5)
  - Test on tablet device (iPad or Android tablet): Layout, touch interactions
  - Test on phone device (iPhone or Android phone): Layout, touch interactions
  - Test portrait and landscape orientations
  - Verify media player controls usable on touch devices
  - Verify click-to-timestamp works with tap gestures
  - Verify inline editing works with on-screen keyboard
  - Use browser DevTools responsive mode for additional viewport testing

- Task 6: Documentation Update (AC: #6)
  - Update project README.md with user instructions
  - Add developer setup instructions (backend + frontend)
  - Document environment requirements (Python 3.12, Node 20, Docker, GPU)
  - Add example .env file with configuration variables
  - Document known limitations and browser compatibility
  - Add troubleshooting section for common issues

- Task 7: Critical Bug Assessment (AC: #7)
  - Review all validation test results
  - Categorize issues: Critical (blocking), Major (workaround exists), Minor (cosmetic)
  - Verify no critical bugs blocking MVP release
  - Document any major/minor issues for post-MVP backlog
  - Create validation summary report with go/no-go recommendation
    </tasks>
  </story>

  <acceptanceCriteria>
1. Complete workflow tested: Upload → Progress → View → Edit → Export on 5+ different media files
2. Cross-browser testing completed: Chrome, Firefox, Safari (desktop + mobile)
3. Error handling verified for all failure scenarios (upload fail, transcription fail, network errors)
4. Performance validated: meets NFR001 targets (load <3s, playback <2s, seek <1s)
5. Mobile responsiveness verified on tablet and phone devices
6. Basic documentation updated: README with user instructions and developer setup
7. No critical bugs blocking MVP release
8. Error scenario testing: file size exceeded (>2GB), network timeout, WhisperX model download failure, concurrent job limits, corrupted media files
9. Safari-specific media seeking behavior tested (Safari playback controls may differ from Chrome/Firefox)
  </acceptanceCriteria>

  <artifacts>
    <docs>
      <doc>
        <path>docs/PRD.md</path>
        <title>KlipNote Product Requirements Document</title>
        <section>Non-Functional Requirements (NFR001-NFR004)</section>
        <snippet>NFR001: Performance - UI load &lt;3s, media playback &lt;2s, seek &lt;1s, transcription 1-2x real-time. NFR002: Usability - Non-technical users complete workflow without documentation. NFR003: Reliability - 90%+ completion rate, browser state prevents data loss. NFR004: Compatibility - Chrome 90+, Firefox 88+, Safari 14+, Edge 90+, mobile browsers, 2GB/2hr file limits.</snippet>
      </doc>
      <doc>
        <path>docs/PRD.md</path>
        <title>KlipNote Product Requirements Document</title>
        <section>User Journey 1: Office Worker Transcribes Meeting</section>
        <snippet>Sarah's complete workflow: Access KlipNote → Upload recording → Monitor progress → Review transcription → Click-to-timestamp navigation → Edit errors → Export results (SRT/TXT) → Use in LLM workflow. Total time: 35 minutes for 45-minute meeting.</snippet>
      </doc>
      <doc>
        <path>docs/architecture.md</path>
        <title>Decision Architecture - KlipNote</title>
        <section>Testing Strategy</section>
        <snippet>Playwright E2E for cross-browser testing, pytest for backend (70%+ coverage), Vitest for frontend (70%+ coverage). Test organization: e2e/tests/ for workflows, error scenarios, performance validation. Mock strategies: WhisperX mock, fakeredis, MSW for API. Cross-browser: Chrome baseline, Firefox media compatibility, Safari Range requests, Edge quick validation.</snippet>
      </doc>
      <doc>
        <path>docs/tech-spec-epic-2.md</path>
        <title>Technical Specification - Epic 2</title>
        <section>Test Strategy Summary</section>
        <snippet>Comprehensive E2E validation for Stories 2.1-2.7. Critical paths: 100% coverage (upload, playback, click-to-timestamp, editing, export). Performance validation against NFR001. Cross-browser testing matrix: Chrome/Firefox/Safari/Edge desktop + mobile. Error scenario testing: network failures, corrupted files, concurrent jobs.</snippet>
      </doc>
      <doc>
        <path>docs/epics.md</path>
        <title>KlipNote - Epic Breakdown</title>
        <section>Story 2.7: MVP Release Checklist &amp; Final Validation</section>
        <snippet>Comprehensive validation story (not typical 2-4 hour story). Tests complete workflow on 5+ files, cross-browser compatibility, error handling, performance (NFR001), mobile responsiveness. Updates documentation. Validates all Epic 1 and Epic 2 work integrates correctly. Produces go/no-go recommendation for MVP release.</snippet>
      </doc>
      <doc>
        <path>docs/architecture.md</path>
        <title>Decision Architecture - KlipNote</title>
        <section>Cross-Cutting Concerns - Error Handling</section>
        <snippet>Backend: HTTPException with 400/404/500 status codes, user-friendly error messages. Frontend: Catch fetch errors, display clear messages without technical jargon or stack traces. Celery tasks catch exceptions, update job status to "failed" with error details.</snippet>
      </doc>
    </docs>
    <code>
      <artifact>
        <path>backend/tests/conftest.py</path>
        <kind>test-config</kind>
        <symbol>pytest fixtures</symbol>
        <lines>all</lines>
        <reason>Test configuration and fixtures for backend pytest suite - reference for E2E test setup patterns</reason>
      </artifact>
      <artifact>
        <path>backend/tests/test_api_endpoints.py</path>
        <kind>test</kind>
        <symbol>API endpoint tests</symbol>
        <lines>all</lines>
        <reason>Existing API tests cover upload, status, result endpoints - validates these work correctly before E2E testing</reason>
      </artifact>
      <artifact>
        <path>backend/tests/test_api_media.py</path>
        <kind>test</kind>
        <symbol>Media API tests</symbol>
        <lines>all</lines>
        <reason>Tests media serving endpoint with Range request support - critical for Safari validation (AC #9)</reason>
      </artifact>
      <artifact>
        <path>backend/tests/test_api_export.py</path>
        <kind>test</kind>
        <symbol>Export API tests</symbol>
        <lines>all</lines>
        <reason>Tests export endpoint and data flywheel - validates backend export functionality before E2E</reason>
      </artifact>
      <artifact>
        <path>backend/app/main.py</path>
        <kind>api</kind>
        <symbol>upload_file, get_status, get_result, serve_media, export_transcription</symbol>
        <lines>all</lines>
        <reason>All API endpoints that need E2E validation - complete workflow depends on these</reason>
      </artifact>
      <artifact>
        <path>frontend/src/views/UploadView.vue</path>
        <kind>component</kind>
        <symbol>UploadView</symbol>
        <lines>all</lines>
        <reason>Entry point for user workflow - E2E tests start here</reason>
      </artifact>
      <artifact>
        <path>frontend/src/views/ProgressView.vue</path>
        <kind>component</kind>
        <symbol>ProgressView</symbol>
        <lines>all</lines>
        <reason>Progress monitoring view - E2E validation of status polling</reason>
      </artifact>
      <artifact>
        <path>frontend/src/views/ResultsView.vue</path>
        <kind>component</kind>
        <symbol>ResultsView</symbol>
        <lines>all</lines>
        <reason>Main review/edit/export interface - E2E validation of click-to-timestamp, editing, export</reason>
      </artifact>
      <artifact>
        <path>frontend/src/components/MediaPlayer.vue</path>
        <kind>component</kind>
        <symbol>MediaPlayer</symbol>
        <lines>all</lines>
        <reason>Media playback component - E2E validation of playback start time (NFR001: &lt;2s), Safari seeking behavior</reason>
      </artifact>
      <artifact>
        <path>frontend/src/components/SubtitleList.vue</path>
        <kind>component</kind>
        <symbol>SubtitleList</symbol>
        <lines>all</lines>
        <reason>Subtitle display and editing - E2E validation of click-to-timestamp (&lt;1s), inline editing, Tab/Escape navigation</reason>
      </artifact>
      <artifact>
        <path>frontend/src/components/ExportModal.vue</path>
        <kind>component</kind>
        <symbol>ExportModal</symbol>
        <lines>all</lines>
        <reason>Export UI component - E2E validation of SRT/TXT format selection and download</reason>
      </artifact>
      <artifact>
        <path>frontend/src/components/ExportModal.test.ts</path>
        <kind>test</kind>
        <symbol>ExportModal component tests</symbol>
        <lines>all</lines>
        <reason>Example of existing frontend component test structure - reference for E2E test patterns</reason>
      </artifact>
      <artifact>
        <path>frontend/src/components/MediaPlayer.test.ts</path>
        <kind>test</kind>
        <symbol>MediaPlayer component tests</symbol>
        <lines>all</lines>
        <reason>Example of media player testing patterns - reference for E2E media playback validation</reason>
      </artifact>
    </code>
    <dependencies>
      <backend>
        <package name="pytest" version="latest">Backend test framework - already configured, validates existing tests pass before E2E</package>
        <package name="pytest-mock" version="latest">Mocking for backend tests - used in existing test suite</package>
        <package name="fastapi" version="0.120.x">API framework - all endpoints need E2E validation</package>
        <package name="celery" version="5.5.3">Async task processing - transcription workflow validation</package>
      </backend>
      <frontend>
        <package name="@playwright/test" version="^1.40.0">NEW - E2E testing framework for cross-browser validation (AC #2)</package>
        <package name="vitest" version="latest">Unit test framework - existing component tests</package>
        <package name="vue" version="3.x">Frontend framework - all components need E2E validation</package>
        <package name="typescript" version="5.x">Type safety - ensures test code is type-safe</package>
      </frontend>
    </dependencies>
  </artifacts>

  <constraints>
    <constraint type="testing-approach">
This is a VALIDATION story, not an implementation story. Focus is comprehensive QA testing of all prior work (Epic 1 + Epic 2 stories). Do NOT implement new features - only create test infrastructure and validation reports.
    </constraint>
    <constraint type="test-organization">
Create e2e/ directory structure: tests/, fixtures/, utils/, playwright.config.ts. Follow architecture pattern: workflow-validation.spec.ts, cross-browser.spec.ts, error-scenarios.spec.ts, performance.spec.ts.
    </constraint>
    <constraint type="playwright-setup">
Install Playwright: npm install --save-dev @playwright/test. Configure for sequential execution (workers: 1) to avoid concurrent job conflicts. Set up web servers for both backend (port 8000) and frontend (port 5173).
    </constraint>
    <constraint type="test-file-preparation">
Create e2e/fixtures/ with test media files: test-short.mp3 (10s), test-medium.mp3 (5min), test-long.mp3 (1hr if available), test-video.mp4 (5min), test-audio.wav (30s), test-corrupted.mp3 (intentionally broken).
    </constraint>
    <constraint type="cross-browser-matrix">
Test on: Chrome 90+ (baseline), Firefox 88+ (media compatibility), Safari 14+ (Range requests, AC #9), Edge 90+ (quick validation), Chrome Mobile (touch interactions), Safari Mobile iOS (touch + seeking).
    </constraint>
    <constraint type="performance-targets">
NFR001 validation required: UI load &lt;3s, media playback start &lt;2s, click-to-timestamp &lt;1s, transcription processing 1-2x real-time (1hr audio = 30-60min). Use Chrome DevTools Performance tab for measurements.
    </constraint>
    <constraint type="error-handling-validation">
All error messages must be user-friendly - no technical stack traces, no internal error codes, clear resolution guidance. Test: unsupported formats, file size exceeded, network timeouts, corrupted files, concurrent job limits.
    </constraint>
    <constraint type="mobile-testing">
Responsive design validation: 320px (iPhone SE) to 1024px (iPad Pro). Touch targets minimum 44x44px. Test portrait and landscape orientations. On-screen keyboard must not obscure content during editing.
    </constraint>
    <constraint type="documentation-requirements">
Update README.md with: User guide (Quick Start, Supported Formats, Export Formats), Developer setup (Prerequisites, Backend/Frontend setup, Environment variables), Browser compatibility, Known limitations, Troubleshooting section.
    </constraint>
    <constraint type="validation-report">
Create validation summary report with: Executive summary (PASS/FAIL, bug counts, GO/NO-GO recommendation), Test results by category (workflow, cross-browser, errors, performance, mobile, docs), Bug categorization (Critical/Major/Minor), Performance metrics table, Browser compatibility table.
    </constraint>
    <constraint type="definition-of-done">
Story complete when: All Playwright E2E tests pass, Validation report generated, All NFR001 targets met, Cross-browser testing completed, Mobile responsiveness validated, Documentation updated, Critical bug assessment shows zero blocking bugs, Go/no-go recommendation documented.
    </constraint>
  </constraints>
  <interfaces>
    <interface>
      <name>POST /upload</name>
      <kind>REST endpoint</kind>
      <signature>POST /upload (multipart/form-data with 'file' field) → {job_id: string}</signature>
      <path>backend/app/main.py:upload_file</path>
      <validation>E2E test: Upload 5+ test files (MP3, MP4, WAV), verify job_id returned. Error test: unsupported format (.exe), file too large (&gt;2GB).</validation>
    </interface>
    <interface>
      <name>GET /status/{job_id}</name>
      <kind>REST endpoint</kind>
      <signature>GET /status/{job_id} → {status: "pending"|"processing"|"completed"|"failed", progress: 0-100, message: string}</signature>
      <path>backend/app/main.py:get_status</path>
      <validation>E2E test: Poll status during transcription, verify progress updates. Error test: invalid job_id returns 404.</validation>
    </interface>
    <interface>
      <name>GET /result/{job_id}</name>
      <kind>REST endpoint</kind>
      <signature>GET /result/{job_id} → {segments: [{start: float, end: float, text: string}]}</signature>
      <path>backend/app/main.py:get_result</path>
      <validation>E2E test: Fetch transcription after completion, verify segments structure. Error test: job not completed returns 404.</validation>
    </interface>
    <interface>
      <name>GET /media/{job_id}</name>
      <kind>REST endpoint</kind>
      <signature>GET /media/{job_id} (with Range header support) → Media file stream (206 Partial Content)</signature>
      <path>backend/app/main.py:serve_media</path>
      <validation>E2E test: Media player loads and plays. Safari-specific (AC #9): Verify Range request seeking works smoothly without buffering stalls.</validation>
    </interface>
    <interface>
      <name>POST /export/{job_id}</name>
      <kind>REST endpoint</kind>
      <signature>POST /export/{job_id} (body: {segments: [...], format: "srt"|"txt"}) → File download</signature>
      <path>backend/app/main.py:export_transcription</path>
      <validation>E2E test: Export both SRT and TXT formats, verify file downloads with correct filename pattern (transcript-{job_id}.{ext}). Verify data flywheel: edited.json and export_metadata.json created in uploads/{job_id}/.</validation>
    </interface>
    <interface>
      <name>MediaPlayer.seekTo(time)</name>
      <kind>Component method</kind>
      <signature>seekTo(time: number) → void (seeks player to timestamp in seconds)</signature>
      <path>frontend/src/components/MediaPlayer.vue</path>
      <validation>E2E test: Click subtitle → verify player.currentTime updates within 1 second (NFR001). Safari-specific: Verify no seeking stutters.</validation>
    </interface>
    <interface>
      <name>SubtitleList click-to-timestamp</name>
      <kind>Component interaction</kind>
      <signature>Click subtitle segment → triggers MediaPlayer.seekTo(segment.start)</signature>
      <path>frontend/src/components/SubtitleList.vue</path>
      <validation>E2E test: Click multiple subtitle segments, verify player seeks correctly each time. Mobile: Test tap gesture on touch devices.</validation>
    </interface>
    <interface>
      <name>localStorage persistence</name>
      <kind>Browser API</kind>
      <signature>localStorage.setItem('klipnote_edits_{job_id}', JSON.stringify({segments, last_saved}))</signature>
      <path>frontend/src/stores/transcription.ts</path>
      <validation>E2E test: Edit subtitle, refresh page, verify edits restored. Test across browser sessions.</validation>
    </interface>
  </interfaces>
  <tests>
    <standards>
Story 2.7 IS the comprehensive testing phase - validates all prior work through E2E testing rather than implementing new features. Testing framework: Playwright for cross-browser E2E tests. Existing backend tests (pytest) and frontend tests (Vitest) should pass before E2E testing begins. E2E test structure: e2e/tests/ directory with workflow-validation.spec.ts, cross-browser.spec.ts, error-scenarios.spec.ts, performance.spec.ts. Playwright configuration: Sequential execution (workers: 1) to avoid concurrent job conflicts, timeout 120s for transcription tests. Performance measurement: Chrome DevTools Performance API, custom timing utilities in e2e/utils/performance.ts. Validation report: Generate markdown report with ValidationReport class tracking PASS/FAIL for each test category.
    </standards>
    <locations>
      <location>e2e/tests/ - NEW Playwright E2E test suite (to be created)</location>
      <location>e2e/fixtures/ - NEW test media files (to be created)</location>
      <location>e2e/utils/ - NEW performance measurement and reporting utilities (to be created)</location>
      <location>backend/tests/ - Existing pytest suite (should pass before E2E)</location>
      <location>frontend/src/components/__tests__/ - Existing Vitest component tests (should pass before E2E)</location>
    </locations>
    <ideas>
      <test ac="1" description="Complete workflow tested on 5+ media files">
        <idea>E2E test: workflow-validation.spec.ts - Upload test-short.mp3 (10s), verify transcription completes, test playback, click-to-timestamp, editing, export SRT/TXT</idea>
        <idea>E2E test: Test workflow on test-medium.mp3 (5min), test-video.mp4 (5min), test-audio.wav (30s) - verify all formats work end-to-end</idea>
        <idea>E2E test: Test workflow on test-long.mp3 (1hr if available) - validate performance target (30-60min transcription time)</idea>
        <idea>Validation: Verify data flywheel - after export, check uploads/{job_id}/ for edited.json and export_metadata.json</idea>
      </test>
      <test ac="2" description="Cross-browser testing completed">
        <idea>E2E test: cross-browser.spec.ts - Run full workflow on Chrome desktop (baseline), verify all features work</idea>
        <idea>E2E test: Run full workflow on Firefox desktop, verify media player compatibility</idea>
        <idea>E2E test: Run full workflow on Edge desktop (quick validation, Chromium-based)</idea>
        <idea>E2E test: Run workflow on Chrome Mobile emulator (Pixel 5 device), verify responsive layout and touch interactions</idea>
      </test>
      <test ac="9" description="Safari-specific media seeking behavior tested">
        <idea>E2E test: cross-browser.spec.ts - Safari desktop test with webkit browser type, verify Range request seeking works smoothly</idea>
        <idea>E2E test: Safari mobile (iPhone 13 emulator) - verify touch tap triggers seek, no buffering stalls during seek operations</idea>
        <idea>Manual test: Real Safari browser on macOS - verify media controls work, seek bar responds smoothly, no Safari-specific quirks</idea>
      </test>
      <test ac="3,8" description="Error handling verified for all failure scenarios">
        <idea>E2E test: error-scenarios.spec.ts - Upload .exe file, verify "File format not supported" error with user-friendly message</idea>
        <idea>E2E test: Simulate network failure during upload (abort route), verify "Network error" message appears</idea>
        <idea>E2E test: Upload corrupted test-corrupted.mp3, verify transcription fails gracefully with clear error message</idea>
        <idea>E2E test: Upload 5 files simultaneously, verify concurrent job handling (queue properly, no crashes)</idea>
        <idea>Manual test: Disconnect network during status polling, verify retry logic and eventual timeout warning</idea>
      </test>
      <test ac="4" description="Performance validated against NFR001 targets">
        <idea>E2E test: performance.spec.ts - Measure UI load time using Performance API, assert &lt;3000ms (NFR001)</idea>
        <idea>E2E test: Measure media playback start time (click play → @playing event), assert &lt;2000ms (NFR001)</idea>
        <idea>E2E test: Measure click-to-timestamp response (click subtitle → player.currentTime updated), assert &lt;1000ms (NFR001)</idea>
        <idea>Manual test: Upload 1hr audio file, monitor backend logs for transcription processing time, verify 30-60min range (1-2x real-time)</idea>
        <idea>Document: Create performance metrics table in validation report with actual measurements vs targets</idea>
      </test>
      <test ac="5" description="Mobile responsiveness verified on tablet and phone">
        <idea>E2E test: cross-browser.spec.ts - Use Playwright device emulation for iPad (768px), verify layout adapts, media player scales</idea>
        <idea>E2E test: Use Playwright device emulation for iPhone 13 (390px), verify single-column layout, export button visible without scroll</idea>
        <idea>E2E test: Test portrait and landscape orientations on mobile devices, verify responsive layout works in both</idea>
        <idea>E2E test: Verify tap gesture on subtitle triggers seek (touch interaction), on-screen keyboard doesn't obscure editing</idea>
        <idea>Manual test: Use browser DevTools responsive mode to test additional viewports (320px iPhone SE, 1024px iPad Pro)</idea>
      </test>
      <test ac="6" description="Basic documentation updated">
        <idea>Task: Update README.md with User Guide section (Quick Start, Supported Formats, Export Formats)</idea>
        <idea>Task: Update README.md with Developer Setup section (Prerequisites, Backend setup, Frontend setup, Environment variables)</idea>
        <idea>Task: Create/update backend/.env.example with CELERY_BROKER_URL, CELERY_RESULT_BACKEND, WHISPER_DEVICE, WHISPER_MODEL</idea>
        <idea>Task: Document browser compatibility (Chrome 90+, Firefox 88+, Safari 14+, Edge 90+) in README.md</idea>
        <idea>Task: Document known limitations (GPU required, CPU mode slower, Safari seeking differences, 2GB/2hr limits) in README.md</idea>
        <idea>Task: Add Troubleshooting section to README.md (common errors and resolutions)</idea>
      </test>
      <test ac="7" description="No critical bugs blocking MVP release">
        <idea>Review: After all E2E tests complete, review test results and categorize any failures</idea>
        <idea>Categorize: Critical bugs (system crashes, data loss, core workflow broken, security issues)</idea>
        <idea>Categorize: Major bugs (feature partially broken, performance worse than NFR001, browser compatibility issues)</idea>
        <idea>Categorize: Minor bugs (UI polish, edge cases, nice-to-haves)</idea>
        <idea>Validate: Confirm zero critical bugs exist - if critical bugs found, mark story incomplete and create fix tasks</idea>
        <idea>Document: Create validation summary report with bug counts, test results, and GO/NO-GO recommendation</idea>
      </test>
    </ideas>
  </tests>
</story-context>
