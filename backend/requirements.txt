# FastAPI and Web Server
fastapi==0.120.0
uvicorn[standard]==0.32.1

# Task Queue
celery[redis]==5.5.3
redis==5.2.1
flower==2.0.1

# Data Validation and Settings
pydantic==2.10.3
pydantic-settings==2.7.0

# File Handling
python-multipart==0.0.20

# Media Processing (wrapper - ffmpeg binary installed via apt-get in Dockerfile)
python-ffmpeg==1.0.16

# Testing
pytest==7.4.4
pytest-mock==3.12.0
pytest-cov==4.1.0
httpx==0.28.1  # For testing FastAPI endpoints
fakeredis==2.21.1  # In-memory Redis for testing

# WhisperX Submodule Dependencies
# These are required by the whisperx git submodule but not installed automatically
# CUDA 12.3 + cuDNN 9 environment enables latest library versions
# - ctranslate2 4.5.0+ now available with cuDNN 9 support
# - faster-whisper 1.1.1+ provides latest Whisper model improvements
# - Version constraints removed to track upstream releases
ctranslate2>=4.5.0
faster-whisper>=1.1.1
nltk>=3.9.1
numpy<2.1.0
onnxruntime
pandas>=2.2.3,<2.3.0
av<16.0.0
# pyannote dependencies REMOVED - using Silero VAD instead
# pyannote.audio has unsolvable dependency conflicts with torch
# Silero VAD (torch.hub model) provides voice activity detection without extra deps
transformers>=4.48.0

# Audio Processing for BELLE-2
# librosa used for audio loading and preprocessing in belle2_service.py
librosa>=0.10.0

# Chinese Language Processing
# Used for Traditionalâ†’Simplified Chinese conversion in transcription post-processing
zhconv>=1.4.3

# Epic 3 - Optimization Pipeline
webrtcvad==2.0.10          # VAD preprocessing (Story 3.2)
pydub==0.25.1              # Audio format conversion (Story 3.2)
scipy==1.11.4              # Signal processing (Story 3.3)
jiwer==3.0.3               # CER/WER calculation (Story 3.5)

# Note: torch, torchvision, and torchaudio are installed in Dockerfile
# with CUDA 12.1-specific index URL: --index-url https://download.pytorch.org/whl/cu121
# CUDA 12.1 wheels are forward-compatible with CUDA 12.3 runtime
# This ensures GPU acceleration is properly configured
